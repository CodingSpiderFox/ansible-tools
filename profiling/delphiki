#!/usr/bin/env python

#
# delphiki:
#   A script to combine and enumerate the outputs of julian
#
# Purpose:
#   Performance troubleshooting ansible can be difficult. If an issue can't
#   be narrowed down to a single task and a single host, this script will
#   aggregate all the debug data into human readable insights.
#
# Usage:
#   1) Run ansible with julian
#       ANSIBLE_VERSION=ansible-2.7.5 ./julian -i inventory -c local site.yml
#   2) Pass the jobresults directory to this script
#       ./delphiki jobresults.2.7.5
#

import argparse
import asciitree
import ast
import csv
import datetime
import glob
import json
import os
import pickle
import pytz
import re
import sys
import sqlite3
import termcolor

from collections import OrderedDict
from pprint import pprint

from logzero import logger
from sh import sed

from sqlalchemy import bindparam
from sqlalchemy import create_engine
from sqlalchemy import select
from sqlalchemy import Table, Boolean, Column, Integer, Float, String, MetaData, ForeignKey


VMSTAT_TIMEZONE = None
LAST_TOP = None
COLORS = ['grey', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white']


def isfloat(a, ignore=None):
    bad = [x for x in a if not x.isdigit() and x != '.']
    if ignore:
        bad = [x for x in bad if x not in ignore]
    if bad:
        return False
    return True

# https://stackoverflow.com/a/19871956
def findkeys(node, kv):
    if isinstance(node, list):
        for i in node:
            for x in findkeys(i, kv):
               yield x
    elif isinstance(node, dict):
        if kv in node:
            yield node[kv]
        for j in node.values():
            for x in findkeys(j, kv):
                yield x


# https://stackoverflow.com/a/12507546
def dict_generator(indict, pre=None):
    pre = pre[:] if pre else []
    if isinstance(indict, dict):
        for key, value in indict.items():
            if isinstance(value, dict) and value:
                for d in dict_generator(value, [key] + pre):
                    yield d
            elif isinstance(value, list) or isinstance(value, tuple):
                for v in value:
                    for d in dict_generator(v, [key] + pre):
                        yield d
            else:
                yield pre + [key, value]
    else:
        yield indict


def pad_string(instring, fill=' ', direction='right', length=None):
    if len(instring) < length:
        diff = length - len(instring)
        diff = fill.join(['' for x in range(0, diff)])
        instring += diff
    return instring


class OnDiskDB(object):
    #DB_ENGINE = {'SQLITE': 'sqlite:///lines.db'}
    engine_url = None
    db_engine = None
    lines = None
    loaded = None
    to_insert = None

    def __init__(self, dbfile=None, dbtype='SQLITE', dbname='linesdb'):
        #engine_url = self.DB_ENGINE[dbtype].format(DB=dbname)
        #self.engine_url = 'sqlite:///' + os.path.abspath(dbfile)
        self.engine_url = 'sqlite://'
        #import epdb; epdb.st()
        self.db_engine = create_engine(self.engine_url)
        self.to_insert = []

    def create_db_tables(self):
        metadata = MetaData()
        self.lines = Table('lines', metadata,
            Column('id', Integer, primary_key=True),
            Column('filename', String),
            Column('linenumber', Integer),
            Column('stdout', Boolean, default=False),
            Column('syslog', Boolean, default=False),
            Column('strace', Boolean, default=False),
            Column('vmstat', Boolean, default=False),
            Column('top', Boolean, default=False),
            Column('pid', Integer),
            Column('ppid', Integer),
            Column('ts', Float),
            Column('duration', Float),
            Column('host', String),
            Column('playbook', String),
            Column('play_name', String),
            Column('task_name', String),
            Column('task_uuid', String),
            Column('task_number', String),
            Column('data', String),
            Column('vmstat_bi', Integer),
            Column('vmstat_bo', Integer),
            Column('vmstat_buff', Integer),
            Column('vmstat_cache', Integer),
            Column('vmstat_cs', Integer),
            Column('vmstat_free', Integer),
            Column('vmstat_id', Integer),
            Column('vmstat_in', Integer),
            Column('vmstat_r', Integer),
            Column('vmstat_si', Integer),
            Column('vmstat_so', Integer),
            Column('vmstat_st', Integer),
            Column('vmstat_swpd', Integer),
            Column('vmstat_sy', Integer),
            Column('vmstat_us', Integer),
            Column('vmstat_wa', Integer),
        )
        metadata.create_all(self.db_engine)

    def add_line(self, data, **kwargs):
        logger.debug('%s:%s' % (data['filename'], data['linenumber']))
        '''
        checkres = self.db_engine.execute(
            "select count(*) from lines where filename = '%s' and linenumber = %s" % (data['filename'], data['linenumber'])
        )
        checkres = [x for x in checkres]
        if checkres[0][0] >= 1:
            return
        '''

        '''
        inskwargs = {}
        for k,v in kwargs.items():
            inskwargs[k] = v
        #import epdb; epdb.st()
        try:
            ins = self.lines.insert().values(**inskwargs)
        except AttributeError as e:
            print(e)
            import epdb; epdb.st()
        '''

        self.to_insert.append(data)

        if len(self.to_insert) >= 50000:
            self.flush_inserts()

        #res = self.db_engine.execute(ins)
        #import epdb; epdb.st()

    def flush_inserts(self, commit=True):
        logger.info('flush to_insert (%s) cache start' % len(self.to_insert))
        colnames = [x.name for x in self.lines.columns]
        if commit:
            trans = self.db_engine.begin()

        for idt,ti in enumerate(self.to_insert):
            inskwargs = {}
            for k,v in ti.items():
                if k not in colnames:
                    continue
                inskwargs[k] = v
            #logger.debug(idt)
            ins = self.lines.insert().values(**inskwargs)
            self.db_engine.execute(ins)

        #import epdb; epdb.st()
        if commit:
            logger.info('flush to_insert cache [commit] start')
            trans.transaction.commit()
            logger.info('flush to_insert cache [commit] done')
        self.to_insert = []
        logger.info('flush to_insert cache done')
        #import epdb; epdb.st()

    def get_lines(self, limit=None):
        s = select([self.lines])
        rows = []
        total = -1
        for row in self.db_engine.execute(s):
            total += 1
            if limit and total > limit:
                break
            rows.append(row)
        return rows


class InMemDB(object):

    '''A SQL+pandas like thingy'''

    odb = None
    odb_file = None
    cachedir = None
    rows = None
    pidmap = None
    pidinfo = None
    known_hosts = None

    _ppids_filled = None
    _filled_ppids = None

    def __init__(self, cachedir=None, usesql=True):
        self.cachedir = cachedir
        self.usesql = usesql
        if self.cachedir and not os.path.exists(self.cachedir):
            os.makedirs(self.cachedir)
        self.odb_file = os.path.join(self.cachedir, 'metadata.db')
        self.init_db()
        self.rows = []
        self.pidmap = {}
        self.pidinfo = {}
        self.known_hosts = set()
        self._filled_ppids = []

    def init_db(self):
        if not os.path.exists(self.odb_file):
            self.odb = OnDiskDB(dbfile=self.odb_file)
            self.odb.create_db_tables()
            self.odb.loaded = False
        else:
            self.odb = OnDiskDB(dbfile=self.odb_file)
            self.odb.loaded = True

    def add_row(self, row):
        self.rows.append(row)
        #if row.get('ts') is not None:
        #    self.odb.add_line(row)
        #if not self.odb.loaded:
        #    self.odb.add_line(row)
        #import epdb; epdb.st()
        if self.usesql:
            self.odb.add_line(row)

    def finalize(self):
        self.odb.flush_inserts()

    def select(self, ppid=None, pid=None, task_name=None, host=None, strace=None, raw=False):
        res = []
        for row in self.rows:
            if ppid and row['ppid'] != ppid:
                continue
            if pid and row['pid'] != pid:
                continue
            if task_name and row['task_name'] != task_name:
                continue
            if host and row['host'] != host:
                continue
            if strace and not row.get('strace', False):
                continue
            res.append(row)

        if raw:
            for idx,x in enumerate(res):
                res[idx]['raw'] = self.get_line_from_file(x['filename'], x['linenumber'])
                #import epdb; epdb.st()

        #import epdb; epdb.st()
        return res

    def update(self, selector, col, val):
        #self.update(('pid': pidnum}, 'host', host)
        for idr,row in enumerate(self.rows):
            match = True
            for k,v in selector.items():
                if row[k] != v:
                    match = False
                    break                
            if match:
                self.rows[idr][col] = val

    def update_pid_meta_file(self, pidnum, key, value):
        import epdb; epdb.st()

    def process(self):
        #self.fill_timestamps()
        self.sort_rows_by_timestamp()
        self.build_pid_map()
        self.build_pid_info()
        self.fill_tasks()
        self.fill_hosts()

    def get_hosts(self):
        hosts = {}
        for x in self.rows:
            if x.get('host') and x['host'] not in hosts:
                hosts[x['host']] = x['ts']
            elif x.get('host') and hosts[x['host']] > x['ts']:
                hosts[x['host']] = x['ts']
        #hosts = [x for x in hosts if x]
        #import epdb; epdb.st()
        hosts = sorted([x[0] for x in hosts.items()], key=lambda y: y)
        return hosts

    def get_line_from_file(self, filename, linenumber):
        # FIXME - sed doesn't seem to work with sh
        #args = "'%sq;d'" % (int(linenumber) + 1)
        args = "%sq;d" % (int(linenumber) + 1)
        #res = sed(args, filename)
        #import epdb; epdb.st()
        return ''

    def fill_hosts(self):
        '''Fill in the host for all pids where the host is known'''
        logger.info('filling in hosts for all pids')
        hosts = self.get_hosts()
        for pid,info in self.pidinfo.items():
            if info.get('host') is not None:
                # fill in the rows
                self.update({'pid': pid}, 'host', info['host'])
                # fill in the meta
                self.update_pid_meta_file(pidnum, 'host', info['host'])

    def fill_tasks(self):
        '''Fill in the task name for pids where ppid has a name'''
        for pid,info in self.pidinfo.items():
            import epdb; epdb.st()

    def fill_timestamps(self, fill='forward'):
        logger.debug('forward filling timestamps')
        for idx, x in enumerate(self.rows):
            if x.get('ts') is None:
                thisidx = idx
                while True:

                    if fill == 'forward':
                        thisidx -= 1
                    elif fill == 'backward':
                        thisidx += 1

                    try:
                        y = self.rows[thisidx]
                    except IndexError as e:
                        break
                    if y['filename'] == x['filename'] and y['ts']:
                        self.rows[idx]['ts'] = y['ts']
                        break

    def fill_ppid(self, pid, ppid):
        logger.debug('fill in pid %s ppid as %s' % (pid, ppid))
        for idx,x in enumerate(self.rows):
            if x['pid'] == pid:
                self.rows[idx]['ppid'] = ppid

    def sort_rows_by_timestamp(self, discard=True):
        if discard:
            self.rows = [x for x in self.rows if x.get('ts') is not None]
        self.rows = sorted(self.rows, key=lambda x: x['ts'])

    def build_pid_map(self):
        logger.debug('building the map of pids')

        '''
        if self.cachedir:
            cfile = os.path.join(self.cachedir, 'pidmap_pids.json')
            if os.path.exists(cfile):
                with open(cfile, 'r') as f:
                    self.pidmap = json.loads(f.read())
                    return
        '''

        #pidmap = {}
        pidmap = OrderedDict()

        # build the tree
        for idx,x in enumerate(self.rows):
            if not x.get('strace'):
                continue

            if x['pid'] and x['clones']:

                if not pidmap or x['pid'] in pidmap:
                    # top pid
                    if x['pid'] not in pidmap:
                        #pidmap[x['pid']] = {}
                        pidmap[x['pid']] = OrderedDict()
                    for clone in x['clones']:

                        # reduce duplicate fills
                        if x['pid'] not in self._filled_ppids:
                            self.fill_ppid(clone, x['pid'])
                            self._filled_ppids.append(x['pid'])

                        if clone not in pidmap[x['pid']]:
                            #pidmap[x['pid']][clone] = {}
                            pidmap[x['pid']][clone] = OrderedDict()
                else:

                    vals = list(findkeys(pidmap, x['pid']))
                    if vals:
                        val = vals[0]
                        for clone in x['clones']:
                            if x['pid'] not in self._filled_ppids:
                                self.fill_ppid(clone, x['pid'])
                                self._filled_ppids.append(x['pid'])
                            if clone not in val:
                                #val[clone] = {}
                                val[clone] = OrderedDict()

                    #import epdb; epdb.st()

        self.pidmap = pidmap.copy()

        if self.cachedir:
            cfile = os.path.join(self.cachedir, 'pidmap_pids.json')
            with open(cfile, 'w') as f:
                f.write(json.dumps(pidmap, indent=2, sort_keys=True))

    def identify_pid(self, pidnum):

        logger.debug('identify %s' % pidnum)

        if self.cachedir:
            cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
            if os.path.exists(cfile):
                with open(cfile, 'r') as f:
                    pidinfo = json.loads(f.read())
                    return pidinfo

        info = {
            'start': None,
            'task_name': None,
            'host': None,
            'stop': None,
            'duration': None,
            'desc': None,
        }

        host = None
        desc = None

        rows = [x for x in self.rows if x['pid'] == pidnum]
        for idp, row in enumerate(rows):
            if not info['start']:
                info['start'] = row['ts']
            if row.get('task_name'):
                info['task_name'] = row['task_name'] 
            if row.get('host'):
                info['host'] = row['host'] 
                host = row['host']

            if row.get('syscall') == 'execve' and row.get('args'):
                args = row['args'][:]
                if args[0].endswith('ansible-playbook'):
                    desc = 'ansible-playbook' 
                elif args[0].endswith('ssh'):
                    desc = ' '.join([args[0], args[-1]])
                elif args[0].endswith('sftp'):
                    desc = 'sftp'
                elif args[0].endswith('scp'):
                    desc = 'scp'
                elif 'bin' in args[0]:
                    desc = ' '.join([args[0], args[-1]])

            #info['stop'] = x['ts']
            info['stop'] = row['ts']

        if host:
            logger.debug('fill in host:%s for pid:%s' % (host, pidnum))
            self.known_hosts.add(host)
            self.update({'pid': pidnum}, 'host', host)
            self.update({'ppid': pidnum}, 'host', host)

        try:
            #info['duration'] = info['stop'] - info['start']
            info['duration'] = rows[-1]['ts'] - rows[0]['ts']
        except TypeError:
            pass

        info['desc'] = desc

        if self.cachedir:
            cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
            with open(cfile, 'w') as f:
                f.write(json.dumps(info))

        return info

    def build_pid_info(self):
        logger.debug('build pids info')
        pid_tuples = list(dict_generator(self.pidmap))
        for pt in pid_tuples:
            for pid in pt:
                if isinstance(pid, dict):
                    continue
                if pid not in self.pidinfo:
                    self.pidinfo[pid] = self.identify_pid(pid)

    def update_pid_meta_file(self, pidnum, key, value):
        cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
        with open(cfile, 'r') as f:
            data = json.loads(f.read())
        data[key] = value
        with open(cfile, 'w') as f:
            f.write(json.dumps(data))

    def print_detailed_tree(self, pidmap=None, level=None, dest=None):

        if pidmap is None:
            pidmap = self.pidmap.copy()
        if level is None:
            level = 0

        # this doesn't work without strace data
        if not pidmap:
            return

        whitelist = ['task_name', 'host', 'duration', 'desc']
        #whitelist = ['task_name', 'host', 'desc']
 
        for k,v in self.pidinfo.items():
            res = list(findkeys(pidmap, k))
            thisdict = res[0]
            items = list(thisdict.items())[:]
            keys = [x[0] for x in items]
            for key in keys:
                thisdict.pop(key, None)

            for wl in whitelist:
                if not v.get(wl):
                    continue
                key = '%s: %s' % (wl, v.get(wl))
                thisdict[key] = {}
            for item in items:
                thisdict[item[0]] = item[1]

        tr = asciitree.LeftAligned()
        try:
            print(tr(pidmap))
        except IndexError as e:
            import epdb; epdb.st()

        if dest:
            with open(dest, 'w') as f:
                f.write(tr(pidmap))

        '''
        for k,v in pidmap.items():
            meta = pidsmeta[k]
            line = ''
            if level == 0:
                line += '%s ansible-playbook %ss' % (pid,meta['duration'])
            import epdb; epdb.st()
        '''

    def graph_fork_timeseries(self, dest=None, timescale=None):
        # use sqllite as much as possible
        '''
        qs = 'select ts,task_name,host from lines' 
        qs += ' where pid IS NOT NULL and task_name IS NOT NULL and host IS NOT NULL' 
        qs += ' order by ts'
        cursor = self.odb.db_engine.execute(qs)
        observations = [list(x) for x in cursor]
        '''

        colnames = [x.name for x in self.odb.lines.columns]
        qs = 'select * from lines where ts IS NOT NULL'
        cursor = self.odb.db_engine.execute(qs)
        obs = []
        for x in cursor:
            data = {}
            for idc,col in enumerate(x):
                data[colnames[idc]] = col
            obs.append(data)
        obs = sorted(obs, key=lambda x: x['ts'])

        obsfile = os.path.join(self.cachedir, 'observations.json')
        with open(obsfile, 'w') as f:
            f.write(json.dumps(obs))

    def print_fork_timeseries(self, dest=None, timescale=None):

        # timescale forces the total time to be static

        width = 350
        t0 = self.rows[0]['ts'] 
        if timescale:
            tN = t0 + float(timescale)
        else:
            tN = self.rows[-1]['ts']
            # strace can hang around long after playbook waiting on controlpersist
            tN = [x for x in self.rows if not x.get('strace')][-1]['ts']

        totalT = tN - t0

        # make a bin for each division of the total time range
        bindiv = totalT / width
        bins = []
        for x in range(0,width):
            if not bins:
                bins.append((t0,t0+bindiv))
            else:
                bins.append((bins[x-1][1], bins[x-1][1] + bindiv)) 

        # map the bins to prepare for checking hosts against each bin
        bindict = OrderedDict()
        for _bin in bins:
            bindict[_bin] = set()

        # check if each host was executing during each of the bins
        #hosts = sorted(self.get_hosts())
        hosts = self.get_hosts()
        host_times = {}

        if self.pidinfo:
            for hn in hosts:
                hpids = [x[0] for x in self.pidinfo.items() if x[1].get('host') == hn]
                hpids_bak = hpids[:]

                for hpid in hpids:
                    for k,v in self.pidinfo.items():
                        if v.get('ppid') == hpid and v.get('host') != hn:
                            self.pidinfo[k]['host'] = hn
                            self.update({'ppid': hpid}, 'host', hn)

                hpids = [x[0] for x in self.pidinfo.items() if x[1].get('host') == hn]
                #import epdb; epdb.st()

                for hpid in hpids:
                    a = self.pidinfo[hpid]['start']
                    b = self.pidinfo[hpid]['stop']
                    binkeys = bindict.keys()

                    for bk in binkeys:
                        if a > bk[1]:
                            continue
                        if a >= bk[0] and a <= bk[1] and b >= bk[1]:
                            bindict[bk].add(hn)
                        if b < bk[0]:
                            break

        else:
            for row in self.rows:            
                if not row.get('host'):
                    continue
                if not row.get('ts'):
                    continue
                hn = row['host']
                ts = row['ts']

                '''
                for k,v in bindict.items():
                    if k[0] >= ts and ts <= k[1]:
                        bindict[k].add(hn)
                    #import epdb; epdb.st()
                '''
                for k,v in bindict.items():
                    if k[0] <= ts <= k[1]:
                        bindict[k].add(hn)

        # when did each task start?
        counter = 0
        task_indexes = {}
        for row in self.rows:
            counter += 1
            if not row.get('task_name'):
                continue
            if not row.get('ts'):
                 continue

            tn = row['task_name']
            ts = row['ts']

            if tn not in task_indexes:
                task_indexes[tn] = ts
                continue

            if task_indexes[tn] > ts:
                task_indexes[tn] = ts

        # fill in the gap between the first task
        task_indexes['play_start'] = [x for x in self.rows if x.get('ts')][0]['ts']

        # order by start time
        task_indexes = sorted(task_indexes.items(), key=lambda x: x[1])

        # reshape with index
        for idx,x in enumerate(task_indexes):
            task_indexes[idx] = [idx, x[0], x[1]]
            hl = max([1+len(x) for x in hosts] + [10])

        # create colormap
        colors = []
        for idx,x in enumerate(task_indexes):
            if not colors:
                colors = COLORS[:]
            task_indexes[idx].append(colors[0])
            colors = colors[1:]
            #import epdb; epdb.st()

        # build titlebar
        title = [pad_string('hostname', length=hl), '|']

        # fill in task start markers
        for ti in task_indexes:
            a = ti[2] 
            try:
                b = task_indexes[ti[0]+1][2]
            except IndexError:
                b = None

            # count how many bins this task lived in
            #bins = [1 for x in bindict.items() if x[0][0] <= t1 and t2 >= x[0][1]]
            #bins = len(bins)
            bins = 0
            inphase = False
            for bk,_hosts in bindict.items():
                if a > bk[1]:
                    continue
                if a >= bk[0] and a <= bk[1]:
                    inphase = True
                    bins += 1
                    continue
                if inphase:
                    #if b is None or bk[0] is None:
                    #    import epdb; epdb.st()
                    if b is None or b < bk[0]:
                        break
                    bins += 1

            if ti[0] == 0:
                coln = ''
            else:
                coln = 't%s' % (ti[0] - 1)
            coln = pad_string(coln, length=bins)
            coln = termcolor.colored(coln, ti[3])
            title.append(coln)


        # pickle for later graphing ...
        data = {
            'bindict':bindict,
            'task_indexes': task_indexes,
            'hosts':hosts,
            'colors': colors
        }
        with open(os.path.join(self.cachedir, 'data.pickle'), 'wb') as f:
            pickle.dump(data, f)

        with open(dest, 'w') as f:

            for ti in task_indexes:
                f.write('# t%s: %s\n' % (ti[0]-1, ti[1]))

            f.write(''.join(title) + '\n')

            for hn in hosts:
                logger.debug('write %s row' % hn)
                _hn = pad_string(hn, length=hl)
                _hn += '|'
                f.write(_hn)
                for bk,bh in bindict.items():

                    # what task are we in right now?
                    color = None
                    for idx,ti in enumerate(task_indexes):
                        if bk[0] <= ti[2] <= bk[1]:
                            color = ti[3]
                            break
                        try:
                            if bk[0] > ti[2] and bk[1] < task_indexes[idx+1][2]:
                                color = ti[3]
                        except IndexError:
                            color = ti[3]

                    if not color:
                        import epdb; epdb.st()

                    if hn in bh:
                        f.write(termcolor.colored('x', color))
                    else:
                        f.write(' ')
                f.write('\n')


def split_executor_line(line, level=None):
    '''Chop all of the info from a taskexecutor log entry'''

    # 2018-10-12 01:29:39,173 p=5489 u=vagrant |    7705 1539307779.17295:
    #   running TaskExecutor() for sshd_145/TASK: Check for /usr/local/sync (Target Directory)
    # 2018-10-12 01:29:39,654 p=5489 u=vagrant |    7591 1539307779.65405:
    #   done running TaskExecutor() for sshd_60/TASK: Check for /usr/local/sync (Target Directory) [525400a6-0421-65e9-9a84-000000000032]
    # 5502 1539307714.25537: done running TaskExecutor() for sshd_250/TASK: wipe out the rules [525400a6-0421-65e9-9a84-00000000002e]

    host = None
    date = None
    time = None
    ts = None
    ppid = None
    pid = None
    uid = None
    uuid = None
    task_name = None
    task = None

    parts = line.split()
    if parts[4] != '|' and not parts[0].isdigit():
        orig_parts = parts[:]
        teidx = parts.index('TaskExecutor()')
        if 'done running TaskExecutor' in line:
            parts = parts[teidx-4:]
        else:
            parts = parts[teidx-3:]
        if not parts[0].isdigit():
            badchars = [x for x in parts[0] if not x.isdigit()]
            #safechars = parts[0].split(badchars[-1])[-1]
            parts[0] = parts[0].split(badchars[-1])[-1]
            #import epdb; epdb.st()

    # sometimes in -syslog- the lines are mixed
    if len(parts) > 20 and level is None:
        starts = re.findall(r'\d+-\d+-\d+\ \d+:\d+:\d+,\d+\ p=\d+\ u=\w+\ \|\W+ ', line)
        if len(starts) > 1:
            newlines = []
            starts = OrderedDict(((x,None) for x in starts))
            for k,v in starts.items():
                starts[k] = line.index(k)
            items = list(starts.items())
            for idi,item in enumerate(starts.items()):
                end = None
                try:
                    end = items[idi+1][1]
                except IndexError:
                    pass
                newlines.append(line[item[1]:end])
            for idnl,nl in enumerate(newlines):
                ldata = split_executor_line(nl, level=1)
                newlines[idnl] = ldata
                #import epdb; epdb.st()
            #print(newlines)
            #import epdb; epdb.st()
            return newlines

    if parts[4] == '|':
        # pylogging
        date = parts[0]
        time = parts[1]
        ppid = int(parts[2].replace('p=', ''))
        uid = parts[3].replace('u=', '')

        # pre 2.8 
        if parts[5].isdigit():
            pid = int(parts[5])
            ts = float(parts[6].replace(':', ''))

        # 2.8
        if parts[5].startswith('<') and parts[5].endswith('>'):
            host = parts[5].replace('<', '').replace('>', '')
            #print(parts[:10])
            #def isfloat(a, ignore=None):
            #import epdb; epdb.st()

        # locate the pid
        if pid is None:
            pids = [x for x in parts if x.isdigit()] 
            if pids:
                pid = int(pids[0])

        # locate the timestamp
        if ts is None:
            timestamps = [x for x in parts if x.endswith(':') and isfloat(x, ignore=':')]
            if timestamps:
                ts = float(timestamps[0].replace(':', ''))

    else:
        # stdout+stderr
        pid = int(parts[0])
        ts = float(parts[1].replace(':', ''))

    if parts[-1].startswith('[') and parts[-1].endswith(']'):
        uuid = parts[-1].replace('[', '').replace(']', '')

    # this is a special place in the line
    for_index = None
    if 'for' in parts:
        for_index = parts.index('for')
        if uuid:
            task = ' '.join(parts[for_index+2:-1])
        else:
            task = ' '.join(parts[for_index+2:])

    # this may have been found earlier
    if host is None and for_index:
        host = parts[for_index+1].split('/', 1)[0]

    return {
        'date': date,
        'time': time,
        'ts': ts,
        'ppid': ppid,
        'pid': pid,
        'uid': uid,
        'uuid': uuid,
        'host': host,
        'task_name': task
    }


def split_ssh_exec(line):
    '''Chop all of the info out of an ssh connection string'''

    # <dockerhost> SSH: EXEC sshpass -d90 ssh -vvv -C -o ControlMaster=auto 
    #   -o ControlPersist=60s -o StrictHostKeyChecking=no -o Port=33017
    #   -o User=root -o ConnectTimeout=10 -o ControlPath=/home/vagrant/.ansible/cp/da9b210846
    #   dockerhost '/bin/sh -c '"'"'echo ~root && sleep 0'"'"''

    parts = line.split()
    hostname = parts[0].replace('<', '').replace('>', '')
    try:
        port = re.search(r"(?<=Port=).*?(?=\ )", line).group(0)
    except:
        port = None

    if 'stricthoskeychecking=no' in line.lower():
        hostkey_checking = False
    else:
        hostkey_checking = True

    if 'sshpass' in line:
        sshpass = True
    else:
        sshpass = False 

    try:
        user = re.search(r"(?<=User=).*?(?=\ )", line).group(0)
    except Exception as e:
        user = None

    if '-o ControlMaster=' in line:
        cp = True
    else:
        cp = False
    try:
        timeout = re.search(r"(?<=ConnectTimeout=).*?(?=\ )", line).group(0)
    except:
        timeout = None
    try:
        cp_path = re.search(r"(?<=ConnectTimeout=).*?(?=\ )", line).group(0)
    except:
        cp_path = None 

    return {
        'hostname': hostname,
        'hostkey_checking': hostkey_checking,
        'port': port,
        'user': user,
        'cp': cp,
        'cp_path': cp_path,
        'sshpass': sshpass,
        'timeout': timeout,
    }


def parse_strace_line(filename, linenumber, line):

    pidnum = int(filename.split('.')[-1])
    ts = re.search(r'\d+\.\d+', line).group()

    syscall = None
    try:
        syscall = re.search(r'\ \w+\(', line).group().replace('(', '').strip()
    except Exception as e:
        # when the master kills children, SIGCHLD is used ...
        # 1549310198.245188 --- SIGCHLD {si_signo=SIGCHLD ...
        if 'SIGCHLD' in line:
            syscall = 'SIGCHLD'
        #import epdb; epdb.st()
        #continue

    duration = line.strip().split()[-1].replace('<', '').replace('>', '')
    try:
        float(duration)
    except Exception as e:
        duration = None

    try:
        cmd = re.search(r'\[.*\],', line).group().rstrip(',')
        cmd = ast.literal_eval(cmd)
    except Exception as e:
        # '1549315066.929309 brk(NULL)             = 0x1b3f000 <0.000006>\n'
        #import epdb; epdb.st()
        cmd = []

    clones = []
    if syscall == 'clone':
        clone = None
        try:
            clone = re.search(r'\ \= \d+', line).group()
            clone = clone.replace('=', '').strip()
            clone = int(clone)
        except AttributeError:
            pass
        if clone not in clones:
            clones.append(clone)
        #import epdb; epdb.st()

    host = None
    if syscall == 'execve' and 'ssh' in str(cmd):
        if len(cmd) > 3:
            if '-L' in cmd:
                ix = cmd.index('-L')
                ix += 1
                host = cmd[ix]
            else:
                host = cmd[-2]
            if '@' in host:
                host = host.split('@')[1]

    data = {
        'clones': clones,
        'ppid': None,
        'pid': pidnum,
        'ts': float(ts),
        'duration': duration,
        'strace': True,
        'syscall': syscall,
        'args': cmd,
        'host': host,
        'filename': filename,
        'linenumber': linenumber,
    }

    return data


def parse_syslog_line(filename, linenumber, line):
    # just get the pid
    #pid = re.search(r'p=\d+', line).group()
    #pid = int(pid.replace('p=', ''))

    pid = None
    ts = None
    try:
        pidts = re.search(r'\d+ \d+\.\d+\:', line).group()
        pid = int(pidts.split()[0])
        ts = float(pidts.split()[1].rstrip(':'))
    except Exception as e:
        pass

    if pid is None:
        pid = re.search(r'p=\d+', line).group()
        pid = int(pid.replace('p=', ''))

    data = {
        'ppid': None,
        'pid': pid,
        'ts': ts,
        'filename': filename,
        'linenumber': linenumber
    }


    # pylogging entries
    if ': running TaskExecutor() for ' in line:
        #data = split_executor_line(line)
        ldata = split_executor_line(line)
        if not isinstance(ldata, list):
            data.update(split_executor_line(line))
        else:
            datasets = []
            for ld in ldata:
                newdata = data.copy()
                newdata.update(ld)
                datasets.append(newdata)
            return datasets

    elif ': done running TaskExecutor() for ' in line:
        ldata = split_executor_line(line)
        if not isinstance(ldata, list):
            data.update(split_executor_line(line))
        else:
            datasets = []
            for ld in ldata:
                newdata = data.copy()
                newdata.update(ld)
                datasets.append(newdata)
            return datasets

    #if 'ansible-playbook' in line:
    #    m = re.search(r'ansible-playbook \d+\.\d+\..*', line)
    #    ansible_version = m.group()

    #if data.get('task_name') and data.get('host'):
    #    print(data)

    return data


def parse_stdout_log(filename, linenumber, line, current_task_name=None):

    task_name = current_task_name
    task_uuid = None
    sshdata = {}
    pid = None
    ts = None

    #if 'Loaded config def from plugin ' in line:
    #    import epdb; epdb.st()

    m = re.search(r'\ \d+\ \d+\.\d+\:', line)
    if m is not None:
        pidts = m.group().strip()
        pid = int(pidts.split()[0])
        ts = float(pidts.split()[1].rstrip(':'))
        #import epdb; epdb.st()

    if line.startswith('TASK'):
        task_name = re.search(r"(?<=TASK \[).*?(?=\])", line).group(0)

    elif 'SSH: EXEC' in line:
        sshdata = split_ssh_exec(line)
        #import epdb; epdb.st()

    elif re.search(r"\ [0-9]+\.[0-9]+\:", line):
        numbers = re.findall(r"[0-9]+", line)

        if 'worker is' in line and 'out of' in line:
            total_forks = int(numbers[-1])

        #ts = float(numbers[1] + '.' + numbers[2])
        pid = int(numbers[0])

    data = {
        'ts': ts,
        'ppid': None,
        'pid': pid,
        'filename': filename,
        'linenumber': linenumber,
        'task_name': task_name,
        'task_uuid': task_uuid,
        'sshdata': sshdata
    }

    return data


def parse_vmstat_line(line):
    # procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
    #  r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
    #  2  0  43836 1524804  31152 278956  148   75   154    83   21    8  0  0 98  2  0 2019-02-11 16:13:04
    #  0  1  2     3        4     5       6    7     8     9    10   11 12 13 14 15 16  17         18
    cols = line.split()
    data = {
        'vmstat_r': int(cols[0]),
        'vmstat_b': int(cols[1]),
        'vmstat_swpd': int(cols[2]),
        'vmstat_free': int(cols[3]),
        'vmstat_buff': int(cols[4]),
        'vmstat_cache': int(cols[5]),
        'vmstat_si': int(cols[6]),
        'vmstat_so': int(cols[7]),
        'vmstat_bi': int(cols[8]),
        'vmstat_bo': int(cols[9]),
        'vmstat_in': int(cols[10]),
        'vmstat_cs': int(cols[11]),
        'vmstat_us': int(cols[12]),
        'vmstat_sy': int(cols[13]),
        'vmstat_id': int(cols[14]),
        'vmstat_wa': int(cols[15]),
        'vmstat_st': int(cols[16]),
    }

    '''
    ts = datetime.datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')
    ts = ts.timestamp()
    data['ts'] = ts
    '''

    ts = cols[17] + ' ' + cols[18]
    tstmp = datetime.datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')

    try:
        tz = getattr(pytz, VMSTAT_TIMEZONE.lower())
    except AttributeError as e:
        tz = pytz.timezone(VMSTAT_TIMEZONE.upper())

    # datetime.datetime(2019,2,11,16,13,4, tzinfo=pytz.utc).timestamp()
    tzts = datetime.datetime(tstmp.year, tstmp.month, tstmp.day, tstmp.hour, tstmp.minute, tstmp.second, tzinfo=tz)

    data['ts'] = tzts.timestamp()

    return data


def parse_top_line(line):
    # top - 16:13:04 up 17 days, 21:39,  1 user,  load average: 0.08, 0.03, 0.05
    # Tasks:  84 total,   2 running,  77 sleeping,   2 stopped,   2 zombie
    # KiB Mem :  1882220 total,  1513532 free,    56916 used,   311772 buff/cache
    # KiB Swap:  2097148 total,  2053312 free,    43836 used.  1597056 avail Mem

    global LAST_TOP

    data = {}
    if line.startswith('top -'):
        cols = line.split()
        data = {
            'users': cols[7],
            'load_1': cols[11],
            'load_5': cols[12],
            'load_15': cols[13],
            'tasks': None,
            'running': None,
            'sleeping': None,
            'stopped': None,
            'zombie': None,
            'kib_mem_total': None,
            'kib_mem_free': None,
            'kib_mem_used': None,
            'kib_mem_buff/cache': None,
        }

        # FIXME - value is negative...
        ts = cols[2]
        ts = datetime.datetime.strptime(ts, '%H:%M:%S')
        ts = ts.timestamp()
        data['ts'] = ts
        LAST_TOP = data.copy()

    elif line.startswith('Tasks:'):
        cols = line.split()
        data = {
            'tasks': int(cols[1]),
            'running': int(cols[3]),
            'sleeping': int(cols[5]),
            'stopped': int(cols[7]),
            'zombie': int(cols[9])
        }
        LAST_TOP.update(data)
        data = LAST_TOP.copy()

    elif line.startswith('KiB Mem'):
        cols = line.split()
        data = {
            'kib_mem_total': int(cols[3]),
            'kib_mem_free': int(cols[5]),
            'kib_mem_used': int(cols[7]),
            'kib_mem_buff/cache': int(cols[9]),
        }
        LAST_TOP.update(data)
        data = LAST_TOP.copy()

    return data


def main():

    global VMSTAT_TIMEZONE

    parser = argparse.ArgumentParser()
    parser.add_argument('--dest', help="DEPRECATED")
    parser.add_argument('--durations-dest', default='/tmp/durations.csv', help="csv file to store the results")
    parser.add_argument('--log-dest', default='/tmp/combined.log', help="log file to store the combined logs")
    parser.add_argument('--nosql', action='store_true')
    parser.add_argument('--nostrace', action='store_true')
    parser.add_argument('--nosyslog', action='store_true')
    parser.add_argument('--nostdout', action='store_true')
    parser.add_argument('--novmstat', action='store_true')
    parser.add_argument('--notop', action='store_true')
    parser.add_argument('--host')
    parser.add_argument('--task')
    parser.add_argument('--pid')
    parser.add_argument('directory')

    args = parser.parse_args()

    assert os.path.isdir(args.directory)

    _fns = []    
    _fns  += glob.glob('%s/*.log' % args.directory)
    _fns += glob.glob('%s/strace*/*' % args.directory) 
    filenames = sorted(set([x for x in _fns if os.path.isfile(x)]))

    DB = InMemDB(cachedir=os.path.join(args.directory, '.cache'), usesql=not args.nosql)

    # found in the logs ...
    ansible_version = None
    total_forks = None

    hostsmeta = {}
    tasks = OrderedDict()
    pids = OrderedDict()
    strace_pids = OrderedDict()
    pidsmeta = {}

    # iterate files and lines to classify and string chop them
    for fn in filenames:
        logger.debug('read %s' % fn)
        with open(fn, 'r') as f:

            # this pops up in the stdout logs once or twice
            current_task_name = None

            lineno = -1
            for line in f.readlines():
                lineno += 1

                if not line.strip():
                    continue

                # strace
                if 'strace' in fn:
                    if not args.nostrace:
                        row = parse_strace_line(fn, lineno, line)
                        row['strace'] = True
                        DB.add_row(row)
                    continue

                # syslog
                if 'p=' in line and 'u=' in line:
                    if not args.nosyslog:
                        if 'ansible-playbook' in line:
                            m = re.search(r'ansible-playbook \d+\.\d+\..*', line)
                            ansible_version = m.group()

                        data = parse_syslog_line(fn, lineno, line)
                        if not isinstance(data, list):
                            data['syslog'] = True
                            DB.add_row(data)
                        else:
                            for dl in data:
                                dl['syslog'] = True
                                DB.add_row(dl)
                    continue

                # vmstat
                if os.path.basename(fn) == 'vmstat.log':
                    if line.startswith('procs ---'):
                        continue
                    if line.lstrip().startswith('r  b'):
                        VMSTAT_TIMEZONE = line.strip().split()[-1]
                        continue
                    if not args.novmstat:
                        data = parse_vmstat_line(line)
                        data['vmstat'] = True
                        data['filename'] = fn
                        data['linenumber'] = lineno
                        DB.add_row(data)
                    continue

                # top
                if os.path.basename(fn) == 'top.log':
                    if not args.notop:
                        data = parse_top_line(line)
                        data['top'] = True
                        data['filename'] = fn
                        data['linenumber'] = lineno
                        if data.get('kib_mem_free') is not None:
                            if data['ts'] > 0:
                                DB.add_row(data)
                    continue

                # stdout
                if not args.nostdout:
                    row = parse_stdout_log(fn, lineno, line, current_task_name=current_task_name)
                    #if row.get('task_name'):
                    #    current_task_name = row['task_name']
                    row['stdout'] = True
                    row['filename'] = fn
                    row['linenumber'] = lineno
                    DB.add_row(row)

    DB.finalize()
    DB.process()
    DB.print_detailed_tree(dest=os.path.join(args.directory, 'pidtree.txt'))
    #DB.print_fork_timeseries(dest=os.path.join(args.directory, 'forktime.txt'))
    #DB.print_fork_timeseries(dest=os.path.join(args.directory, 'forktime_abs.txt'), timescale=125)
    DB.graph_fork_timeseries(dest=os.path.join(args.directory, 'forktime_abs.txt'), timescale=125)
    #DB.select(ppid=None, pid=5976, task_name=None, host=None, strace=None, raw=True)

    #import epdb; epdb.st()


if __name__ == "__main__":
    main()
