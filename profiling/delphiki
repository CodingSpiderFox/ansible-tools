#!/usr/bin/env python

#
# delphiki:
#   A script to combine and enumerate the outputs of julian
#
# Purpose:
#   Performance troubleshooting ansible can be difficult. If an issue can't
#   be narrowed down to a single task and a single host, this script will
#   aggregate all the debug data into human readable insights.
#
# Usage:
#   1) Run ansible with julian
#       ANSIBLE_VERSION=ansible-2.7.5 ./julian -i inventory -c local site.yml
#   2) Pass the jobresults directory to this script
#       ./delphiki jobresults.2.7.5
#

import argparse
import asciitree
import ast
import csv
import glob
import json
import os
import pickle
import re
import sys
import sqlite3
import termcolor

from collections import OrderedDict
from pprint import pprint

from logzero import logger
from sh import sed

from sqlalchemy import create_engine
from sqlalchemy import select
from sqlalchemy import Table, Boolean, Column, Integer, Float, String, MetaData, ForeignKey
#conn = sqlite3.connect('parsed.db')
#curs = conn.cursor()


COLORS = ['grey', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white']


# https://stackoverflow.com/a/19871956
def findkeys(node, kv):
    if isinstance(node, list):
        for i in node:
            for x in findkeys(i, kv):
               yield x
    elif isinstance(node, dict):
        if kv in node:
            yield node[kv]
        for j in node.values():
            for x in findkeys(j, kv):
                yield x


# https://stackoverflow.com/a/12507546
def dict_generator(indict, pre=None):
    pre = pre[:] if pre else []
    if isinstance(indict, dict):
        for key, value in indict.items():
            if isinstance(value, dict) and value:
                for d in dict_generator(value, [key] + pre):
                    yield d
            elif isinstance(value, list) or isinstance(value, tuple):
                for v in value:
                    for d in dict_generator(v, [key] + pre):
                        yield d
            else:
                yield pre + [key, value]
    else:
        yield indict


def pad_string(instring, fill=' ', direction='right', length=None):
    if len(instring) < length:
        diff = length - len(instring)
        diff = fill.join(['' for x in range(0, diff)])
        instring += diff
    return instring


class ParserDB(object):
    DB_ENGINE = {'SQLITE': 'sqlite:///lines.db'}
    db_engine = None
    lines = None

    def __init__(self, dbtype='SQLITE', dbname='linesdb'):
        engine_url = self.DB_ENGINE[dbtype].format(DB=dbname)
        self.db_engine = create_engine(engine_url)

    def create_db_tables(self):
        metadata = MetaData()
        self.lines = Table('lines', metadata,
            Column('id', Integer, primary_key=True),
            Column('filename', String),
            Column('linenumber', Integer),
            Column('stdout', Boolean, default=False),
            Column('syslog', Boolean, default=False),
            Column('strace', Boolean, default=False),
            Column('pid', Integer),
            Column('ppid', Integer),
            Column('ts', Float),
            Column('duration', Float),
            Column('host', String),
            Column('data', String)
        )
        metadata.create_all(self.db_engine)

    def add_line(self, data, **kwargs):
        inskwargs = {}
        for k,v in kwargs.items():
            inskwargs[k] = v
        ins = self.lines.insert().values(data=data, **inskwargs)
        res = self.db_engine.execute(ins)
        #import epdb; epdb.st()

    def get_lines(self, limit=None):
        s = select([self.lines])
        rows = []
        total = -1
        for row in self.db_engine.execute(s):
            total += 1
            if limit and total > limit:
                break
            rows.append(row)
        return rows


class InMemDB(object):

    '''A SQL+pandas like thingy'''

    cachedir = None
    rows = None
    pidmap = None
    pidinfo = None
    known_hosts = None

    _ppids_filled = None
    _filled_ppids = None

    def __init__(self, cachedir=None):
        self.cachedir = cachedir
        if self.cachedir and not os.path.exists(self.cachedir):
            os.makedirs(self.cachedir)
        self.rows = []
        self.pidmap = {}
        self.pidinfo = {}
        self.known_hosts = set()
        self._filled_ppids = []

    def add_row(self, row):
        self.rows.append(row)

    def select(self, ppid=None, pid=None, task_name=None, host=None, strace=None, raw=False):
        res = []
        for row in self.rows:
            if ppid and row['ppid'] != ppid:
                continue
            if pid and row['pid'] != pid:
                continue
            if task_name and row['task_name'] != task_name:
                continue
            if host and row['host'] != host:
                continue
            if strace and not row.get('strace', False):
                continue
            res.append(row)

        if raw:
            for idx,x in enumerate(res):
                res[idx]['raw'] = self.get_line_from_file(x['filename'], x['linenumber'])
                #import epdb; epdb.st()

        #import epdb; epdb.st()
        return res

    def update(self, selector, col, val):
        #self.update(('pid': pidnum}, 'host', host)
        for idr,row in enumerate(self.rows):
            match = True
            for k,v in selector.items():
                if row[k] != v:
                    match = False
                    break                
            if match:
                self.rows[idr][col] = val

    def update_pid_meta_file(self, pidnum, key, value):
        import epdb; epdb.st()

    def process(self):
        #self.fill_timestamps()
        self.sort_rows_by_timestamp()
        self.build_pid_map()
        self.build_pid_info()
        #self.fill_hosts()
        import epdb; epdb.st()

    def get_hosts(self):
        hosts = {}
        for x in self.rows:
            if x.get('host') and x['host'] not in hosts:
                hosts[x['host']] = x['ts']
            elif x.get('host') and hosts[x['host']] > x['ts']:
                hosts[x['host']] = x['ts']
        #hosts = [x for x in hosts if x]
        #import epdb; epdb.st()
        hosts = sorted([x[0] for x in hosts.items()], key=lambda y: y)
        return hosts

    def get_line_from_file(self, filename, linenumber):
        # FIXME - sed doesn't seem to work with sh
        args = "'%sq;d'" % (int(linenumber) + 1)
        #res = sed(args, filename)
        #import epdb; epdb.st()
        return ''

    def fill_hosts(self):
        '''Fill in the host for all pids where the host is known'''
        logger.info('filling in hosts for all pids')
        hosts = self.get_hosts()
        for pid,info in self.pidinfo.items():
            if info.get('host') is not None:
                self.update({'pid': pid}, 'host', info['host'])

    def fill_timestamps(self, fill='forward'):
        logger.debug('forward filling timestamps')
        for idx, x in enumerate(self.rows):
            if x.get('ts') is None:
                thisidx = idx
                while True:

                    if fill == 'forward':
                        thisidx -= 1
                    elif fill == 'backward':
                        thisidx += 1

                    try:
                        y = self.rows[thisidx]
                    except IndexError as e:
                        break
                    if y['filename'] == x['filename'] and y['ts']:
                        self.rows[idx]['ts'] = y['ts']
                        break

    def fill_ppid(self, pid, ppid):
        logger.debug('fill in pid %s ppid as %s' % (pid, ppid))
        for idx,x in enumerate(self.rows):
            if x['pid'] == pid:
                self.rows[idx]['ppid'] = ppid

    def sort_rows_by_timestamp(self, discard=True):
        if discard:
            self.rows = [x for x in self.rows if x.get('ts') is not None]
        self.rows = sorted(self.rows, key=lambda x: x['ts'])

    def build_pid_map(self):
        logger.debug('building the map of pids')

        '''
        if self.cachedir:
            cfile = os.path.join(self.cachedir, 'pidmap_pids.json')
            if os.path.exists(cfile):
                with open(cfile, 'r') as f:
                    self.pidmap = json.loads(f.read())
                    return
        '''

        #pidmap = {}
        pidmap = OrderedDict()

        # build the tree
        for idx,x in enumerate(self.rows):
            if not x.get('strace'):
                continue

            if x['pid'] and x['clones']:

                if not pidmap or x['pid'] in pidmap:
                    # top pid
                    if x['pid'] not in pidmap:
                        #pidmap[x['pid']] = {}
                        pidmap[x['pid']] = OrderedDict()
                    for clone in x['clones']:

                        # reduce duplicate fills
                        if x['pid'] not in self._filled_ppids:
                            self.fill_ppid(clone, x['pid'])
                            self._filled_ppids.append(x['pid'])

                        if clone not in pidmap[x['pid']]:
                            #pidmap[x['pid']][clone] = {}
                            pidmap[x['pid']][clone] = OrderedDict()
                else:

                    vals = list(findkeys(pidmap, x['pid']))
                    if vals:
                        val = vals[0]
                        for clone in x['clones']:
                            if x['pid'] not in self._filled_ppids:
                                self.fill_ppid(clone, x['pid'])
                                self._filled_ppids.append(x['pid'])
                            if clone not in val:
                                #val[clone] = {}
                                val[clone] = OrderedDict()

                    #import epdb; epdb.st()

        self.pidmap = pidmap.copy()

        if self.cachedir:
            cfile = os.path.join(self.cachedir, 'pidmap_pids.json')
            with open(cfile, 'w') as f:
                f.write(json.dumps(pidmap, indent=2, sort_keys=True))

    def identify_pid(self, pidnum):

        logger.debug('identify %s' % pidnum)

        if self.cachedir:
            cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
            if os.path.exists(cfile):
                with open(cfile, 'r') as f:
                    pidinfo = json.loads(f.read())
                    return pidinfo

        info = {
            'start': None,
            'task_name': None,
            'host': None,
            'stop': None,
            'duration': None,
            'desc': None,
        }

        host = None
        desc = None

        rows = [x for x in self.rows if x['pid'] == pidnum]
        for idp, row in enumerate(rows):
            if not info['start']:
                info['start'] = row['ts']
            if row.get('task_name'):
                info['task_name'] = row['task_name'] 
            if row.get('host'):
                info['host'] = row['host'] 
                host = row['host']

            if row.get('syscall') == 'execve' and row.get('args'):
                args = row['args'][:]
                if args[0].endswith('ansible-playbook'):
                    desc = 'ansible-playbook' 
                elif args[0].endswith('ssh'):
                    desc = ' '.join([args[0], args[-1]])
                elif args[0].endswith('sftp'):
                    desc = 'sftp'
                elif args[0].endswith('scp'):
                    desc = 'scp'
                elif 'bin' in args[0]:
                    desc = ' '.join([args[0], args[-1]])

            #info['stop'] = x['ts']
            info['stop'] = row['ts']

        if host:
            logger.debug('fill in host:%s for pid:%s' % (host, pidnum))
            self.known_hosts.add(host)
            self.update({'pid': pidnum}, 'host', host)
            self.update({'ppid': pidnum}, 'host', host)

        try:
            #info['duration'] = info['stop'] - info['start']
            info['duration'] = rows[-1]['ts'] - rows[0]['ts']
        except TypeError:
            pass

        info['desc'] = desc

        if self.cachedir:
            cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
            with open(cfile, 'w') as f:
                f.write(json.dumps(info))

        return info

    def build_pid_info(self):
        logger.debug('build pids info')
        pid_tuples = list(dict_generator(self.pidmap))
        for pt in pid_tuples:
            for pid in pt:
                if isinstance(pid, dict):
                    continue
                if pid not in self.pidinfo:
                    self.pidinfo[pid] = self.identify_pid(pid)

    def update_pid_meta_file(self, pidnum, key, value):
        cfile = os.path.join(self.cachedir, '%s.info' % pidnum)
        with open(cfile, 'r') as f:
            data = json.loads(f.read())
        data[key] = value
        with open(cfile, 'w') as f:
            f.write(json.dumps(data))

    def print_detailed_tree(self, pidmap=None, level=None, dest=None):

        if pidmap is None:
            pidmap = self.pidmap.copy()
        if level is None:
            level = 0

        # this doesn't work without strace data
        if not pidmap:
            return

        whitelist = ['task_name', 'host', 'duration', 'desc']
        #whitelist = ['task_name', 'host', 'desc']
 
        for k,v in self.pidinfo.items():
            res = list(findkeys(pidmap, k))
            thisdict = res[0]
            items = list(thisdict.items())[:]
            keys = [x[0] for x in items]
            for key in keys:
                thisdict.pop(key, None)

            for wl in whitelist:
                if not v.get(wl):
                    continue
                key = '%s: %s' % (wl, v.get(wl))
                thisdict[key] = {}
            for item in items:
                thisdict[item[0]] = item[1]

        tr = asciitree.LeftAligned()
        try:
            print(tr(pidmap))
        except IndexError as e:
            import epdb; epdb.st()

        if dest:
            with open(dest, 'w') as f:
                f.write(tr(pidmap))

        '''
        for k,v in pidmap.items():
            meta = pidsmeta[k]
            line = ''
            if level == 0:
                line += '%s ansible-playbook %ss' % (pid,meta['duration'])
            import epdb; epdb.st()
        '''

    def print_fork_timeseries(self, dest=None, timescale=None):

        # timescale forces the total time to be static

        width = 350
        t0 = self.rows[0]['ts'] 
        if timescale:
            tN = t0 + float(timescale)
        else:
            tN = self.rows[-1]['ts']
            # strace can hang around long after playbook waiting on controlpersist
            tN = [x for x in self.rows if not x.get('strace')][-1]['ts']

        totalT = tN - t0

        # make a bin for each division of the total time range
        bindiv = totalT / width
        bins = []
        for x in range(0,width):
            if not bins:
                bins.append((t0,t0+bindiv))
            else:
                bins.append((bins[x-1][1], bins[x-1][1] + bindiv)) 

        # map the bins to prepare for checking hosts against each bin
        bindict = OrderedDict()
        for _bin in bins:
            bindict[_bin] = set()

        # check if each host was executing during each of the bins
        #hosts = sorted(self.get_hosts())
        hosts = self.get_hosts()
        host_times = {}

        if self.pidinfo:
            for hn in hosts:
                hpids = [x[0] for x in self.pidinfo.items() if x[1].get('host') == hn]
                hpids_bak = hpids[:]

                for hpid in hpids:
                    for k,v in self.pidinfo.items():
                        if v.get('ppid') == hpid and v.get('host') != hn:
                            self.pidinfo[k]['host'] = hn
                            self.update({'ppid': hpid}, 'host', hn)

                hpids = [x[0] for x in self.pidinfo.items() if x[1].get('host') == hn]
                #import epdb; epdb.st()

                for hpid in hpids:
                    a = self.pidinfo[hpid]['start']
                    b = self.pidinfo[hpid]['stop']
                    binkeys = bindict.keys()

                    for bk in binkeys:
                        if a > bk[1]:
                            continue
                        if a >= bk[0] and a <= bk[1] and b >= bk[1]:
                            bindict[bk].add(hn)
                        if b < bk[0]:
                            break

        else:
            for row in self.rows:            
                if not row.get('host'):
                    continue
                if not row.get('ts'):
                    continue
                hn = row['host']
                ts = row['ts']

                '''
                for k,v in bindict.items():
                    if k[0] >= ts and ts <= k[1]:
                        bindict[k].add(hn)
                    #import epdb; epdb.st()
                '''
                for k,v in bindict.items():
                    if k[0] <= ts <= k[1]:
                        bindict[k].add(hn)

        # when did each task start?
        counter = 0
        task_indexes = {}
        for row in self.rows:
            counter += 1
            if not row.get('task_name'):
                continue
            if not row.get('ts'):
                 continue

            tn = row['task_name']
            ts = row['ts']

            if tn not in task_indexes:
                task_indexes[tn] = ts
                continue

            if task_indexes[tn] > ts:
                task_indexes[tn] = ts

        # fill in the gap between the first task
        task_indexes['play_start'] = [x for x in self.rows if x.get('ts')][0]['ts']

        # order by start time
        task_indexes = sorted(task_indexes.items(), key=lambda x: x[1])

        # reshape with index
        for idx,x in enumerate(task_indexes):
            task_indexes[idx] = [idx, x[0], x[1]]
            hl = max([1+len(x) for x in hosts] + [10])

        # create colormap
        colors = []
        for idx,x in enumerate(task_indexes):
            if not colors:
                colors = COLORS[:]
            task_indexes[idx].append(colors[0])
            colors = colors[1:]
            #import epdb; epdb.st()

        # build titlebar
        title = [pad_string('hostname', length=hl), '|']

        # fill in task start markers
        for ti in task_indexes:
            a = ti[2] 
            try:
                b = task_indexes[ti[0]+1][2]
            except IndexError:
                b = None

            # count how many bins this task lived in
            #bins = [1 for x in bindict.items() if x[0][0] <= t1 and t2 >= x[0][1]]
            #bins = len(bins)
            bins = 0
            inphase = False
            for bk,_hosts in bindict.items():
                if a > bk[1]:
                    continue
                if a >= bk[0] and a <= bk[1]:
                    inphase = True
                    bins += 1
                    continue
                if inphase:
                    #if b is None or bk[0] is None:
                    #    import epdb; epdb.st()
                    if b is None or b < bk[0]:
                        break
                    bins += 1

            if ti[0] == 0:
                coln = ''
            else:
                coln = 't%s' % (ti[0] - 1)
            coln = pad_string(coln, length=bins)
            coln = termcolor.colored(coln, ti[3])
            title.append(coln)


        data = {
            'bindict':bindict,
            'task_indexes': task_indexes,
            'hosts':hosts,
            'colors': colors
        }
        with open('/tmp/data.pickle', 'wb') as f:
            pickle.dump(data, f)

        # def pad_string(instring, fill=' ', direction='right', length=None):
        with open(dest, 'w') as f:

            for ti in task_indexes:
                f.write('# t%s: %s\n' % (ti[0]-1, ti[1]))
            #import epdb; epdb.st()

            f.write(''.join(title) + '\n')

            for hn in hosts:
                logger.debug('write %s row' % hn)
                _hn = pad_string(hn, length=hl)
                _hn += '|'
                f.write(_hn)
                for bk,bh in bindict.items():

                    # what task are we in right now?
                    color = None
                    for idx,ti in enumerate(task_indexes):
                        if bk[0] <= ti[2] <= bk[1]:
                            color = ti[3]
                            break
                        try:
                            if bk[0] > ti[2] and bk[1] < task_indexes[idx+1][2]:
                                color = ti[3]
                        except IndexError:
                            color = ti[3]

                    if not color:
                        import epdb; epdb.st()

                    if hn in bh:
                        f.write(termcolor.colored('x', color))
                    else:
                        f.write(' ')
                f.write('\n')

        #import epdb; epdb.st()



def split_executor_line(line):
    '''Chop all of the info from a taskexecutor log entry'''

    # 2018-10-12 01:29:39,173 p=5489 u=vagrant |    7705 1539307779.17295:
    #   running TaskExecutor() for sshd_145/TASK: Check for /usr/local/sync (Target Directory)
    # 2018-10-12 01:29:39,654 p=5489 u=vagrant |    7591 1539307779.65405:
    #   done running TaskExecutor() for sshd_60/TASK: Check for /usr/local/sync (Target Directory) [525400a6-0421-65e9-9a84-000000000032]
    # 5502 1539307714.25537: done running TaskExecutor() for sshd_250/TASK: wipe out the rules [525400a6-0421-65e9-9a84-00000000002e]

    parts = line.split()
    if parts[4] != '|' and not parts[0].isdigit():
        orig_parts = parts[:]
        teidx = parts.index('TaskExecutor()')
        if 'done running TaskExecutor' in line:
            parts = parts[teidx-4:]
        else:
            parts = parts[teidx-3:]
        if not parts[0].isdigit():
            badchars = [x for x in parts[0] if not x.isdigit()]
            #safechars = parts[0].split(badchars[-1])[-1]
            parts[0] = parts[0].split(badchars[-1])[-1]
            #import epdb; epdb.st()

    if parts[4] == '|':
        # pylogging
        date = parts[0]
        time = parts[1]
        ppid = int(parts[2].replace('p=', ''))
        uid = parts[3].replace('u=', '')
        pid = int(parts[5])
        ts = float(parts[6].replace(':', ''))
    else:
        # stdout+stderr
        date = None
        time = None
        ppid = None
        uid = None
        try:
            pid = int(parts[0])
        except Exception as e:
            print(e)
            import epdb; epdb.st()
        ts = float(parts[1].replace(':', ''))

    uuid = None
    if parts[-1].startswith('[') and parts[-1].endswith(']'):
        uuid = parts[-1].replace('[', '').replace(']', '')

    for_index = parts.index('for')
    host = parts[for_index+1].split('/', 1)[0]

    if uuid:
        task = ' '.join(parts[for_index+2:-1])
    else:
        task = ' '.join(parts[for_index+2:])

    return {
        'date': date,
        'time': time,
        'ts': ts,
        'ppid': ppid,
        'pid': pid,
        'uid': uid,
        'uuid': uuid,
        'host': host,
        'task_name': task
    }


def split_ssh_exec(line):
    '''Chop all of the info out of an ssh connection string'''

    # <dockerhost> SSH: EXEC sshpass -d90 ssh -vvv -C -o ControlMaster=auto 
    #   -o ControlPersist=60s -o StrictHostKeyChecking=no -o Port=33017
    #   -o User=root -o ConnectTimeout=10 -o ControlPath=/home/vagrant/.ansible/cp/da9b210846
    #   dockerhost '/bin/sh -c '"'"'echo ~root && sleep 0'"'"''

    parts = line.split()
    hostname = parts[0].replace('<', '').replace('>', '')
    try:
        port = re.search(r"(?<=Port=).*?(?=\ )", line).group(0)
    except:
        port = None

    if 'stricthoskeychecking=no' in line.lower():
        hostkey_checking = False
    else:
        hostkey_checking = True

    if 'sshpass' in line:
        sshpass = True
    else:
        sshpass = False 

    try:
        user = re.search(r"(?<=User=).*?(?=\ )", line).group(0)
    except Exception as e:
        user = None

    if '-o ControlMaster=' in line:
        cp = True
    else:
        cp = False
    try:
        timeout = re.search(r"(?<=ConnectTimeout=).*?(?=\ )", line).group(0)
    except:
        timeout = None
    try:
        cp_path = re.search(r"(?<=ConnectTimeout=).*?(?=\ )", line).group(0)
    except:
        cp_path = None 

    return {
        'hostname': hostname,
        'hostkey_checking': hostkey_checking,
        'port': port,
        'user': user,
        'cp': cp,
        'cp_path': cp_path,
        'sshpass': sshpass,
        'timeout': timeout,
    }


def parse_strace_line(filename, linenumber, line):

    pidnum = int(filename.split('.')[-1])
    ts = re.search(r'\d+\.\d+', line).group()

    syscall = None
    try:
        syscall = re.search(r'\ \w+\(', line).group().replace('(', '').strip()
    except Exception as e:
        # when the master kills children, SIGCHLD is used ...
        # 1549310198.245188 --- SIGCHLD {si_signo=SIGCHLD ...
        if 'SIGCHLD' in line:
            syscall = 'SIGCHLD'
        #import epdb; epdb.st()
        #continue

    duration = line.strip().split()[-1].replace('<', '').replace('>', '')
    try:
        float(duration)
    except Exception as e:
        duration = None

    try:
        cmd = re.search(r'\[.*\],', line).group().rstrip(',')
        cmd = ast.literal_eval(cmd)
    except Exception as e:
        # '1549315066.929309 brk(NULL)             = 0x1b3f000 <0.000006>\n'
        #import epdb; epdb.st()
        cmd = []

    clones = []
    if syscall == 'clone':
        clone = None
        try:
            clone = re.search(r'\ \= \d+', line).group()
            clone = clone.replace('=', '').strip()
            clone = int(clone)
        except AttributeError:
            pass
        if clone not in clones:
            clones.append(clone)
        #import epdb; epdb.st()

    host = None
    if syscall == 'execve' and 'ssh' in str(cmd):
        if len(cmd) > 3:
            if '-L' in cmd:
                ix = cmd.index('-L')
                ix += 1
                host = cmd[ix]
            else:
                host = cmd[-2]
            if '@' in host:
                host = host.split('@')[1]

    data = {
        'clones': clones,
        'ppid': None,
        'pid': pidnum,
        'ts': float(ts),
        'duration': duration,
        'strace': True,
        'syscall': syscall,
        'args': cmd,
        'host': host,
        'filename': filename,
        'linenumber': linenumber,
    }

    return data


def parse_syslog_line(filename, linenumber, line):
    # just get the pid
    #pid = re.search(r'p=\d+', line).group()
    #pid = int(pid.replace('p=', ''))

    pid = None
    ts = None
    try:
        pidts = re.search(r'\d+ \d+\.\d+\:', line).group()
        pid = int(pidts.split()[0])
        ts = float(pidts.split()[1].rstrip(':'))
    except Exception as e:
        pass

    if pid is None:
        pid = re.search(r'p=\d+', line).group()
        pid = int(pid.replace('p=', ''))

    data = {
        'ppid': None,
        'pid': pid,
        'ts': ts,
        'filename': filename,
        'linenumber': linenumber
    }


    # pylogging entries
    if ': running TaskExecutor() for ' in line:
        #data = split_executor_line(line)
        data.update(split_executor_line(line))

    elif ': done running TaskExecutor() for ' in line:
        data.update(split_executor_line(line))

    #if 'ansible-playbook' in line:
    #    m = re.search(r'ansible-playbook \d+\.\d+\..*', line)
    #    ansible_version = m.group()

    #if data.get('task_name') and data.get('host'):
    #    print(data)

    return data


def parse_stdout_log(filename, linenumber, line, current_task_name=None):

    task_name = current_task_name
    task_uuid = None
    sshdata = {}
    pid = None
    ts = None

    #if 'Loaded config def from plugin ' in line:
    #    import epdb; epdb.st()

    m = re.search(r'\ \d+\ \d+\.\d+\:', line)
    if m is not None:
        pidts = m.group().strip()
        pid = int(pidts.split()[0])
        ts = float(pidts.split()[1].rstrip(':'))
        #import epdb; epdb.st()

    if line.startswith('TASK'):
        task_name = re.search(r"(?<=TASK \[).*?(?=\])", line).group(0)

    elif 'SSH: EXEC' in line:
        sshdata = split_ssh_exec(line)
        #import epdb; epdb.st()

    elif re.search(r"\ [0-9]+\.[0-9]+\:", line):
        numbers = re.findall(r"[0-9]+", line)

        if 'worker is' in line and 'out of' in line:
            total_forks = int(numbers[-1])

        #ts = float(numbers[1] + '.' + numbers[2])
        pid = int(numbers[0])

    data = {
        'ts': ts,
        'ppid': None,
        'pid': pid,
        'filename': filename,
        'linenumber': linenumber,
        'task_name': task_name,
        'task_uuid': task_uuid,
        'sshdata': sshdata
    }

    return data


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('--dest', help="DEPRECATED")
    parser.add_argument('--durations-dest', default='/tmp/durations.csv', help="csv file to store the results")
    parser.add_argument('--log-dest', default='/tmp/combined.log', help="log file to store the combined logs")
    parser.add_argument('--host')
    parser.add_argument('--task')
    parser.add_argument('--pid')
    parser.add_argument('directory')

    args = parser.parse_args()

    assert os.path.isdir(args.directory)

    _fns = []    
    _fns  += glob.glob('%s/*.log' % args.directory)
    _fns += glob.glob('%s/strace*/*' % args.directory) 
    filenames = sorted(set([x for x in _fns if os.path.isfile(x)]))
    #import epdb; epdb.st()


    '''
    DB = ParserDB()
    DB.create_db_tables()
    DB.add_line(filename='foo.txt', linenumber=3, data='foobar')
    '''

    DB = InMemDB(cachedir=os.path.join(args.directory, '.cache'))
    #import epdb; epdb.st()

    # found in the logs ...
    ansible_version = None
    total_forks = None

    hostsmeta = {}
    tasks = OrderedDict()
    pids = OrderedDict()
    strace_pids = OrderedDict()
    pidsmeta = {}

    # iterate files and lines to classify and string chop them
    for fn in filenames:
        logger.debug('read %s' % fn)
        with open(fn, 'r') as f:

            # this pops up in the stdout logs once or twice
            current_task_name = None

            lineno = -1
            for line in f.readlines():
                lineno += 1

                if not line.strip():
                    continue

                # strace
                if 'strace' in fn:
                    row = parse_strace_line(fn, lineno, line)
                    DB.add_row(row)
                    continue

                # syslog
                if 'p=' in line and 'u=' in line:

                    if 'ansible-playbook' in line:
                        m = re.search(r'ansible-playbook \d+\.\d+\..*', line)
                        ansible_version = m.group()

                    data = parse_syslog_line(fn, lineno, line)
                    DB.add_row(data)
                    continue

                # stdout
                row = parse_stdout_log(fn, lineno, line, current_task_name=current_task_name)
                if row.get('task_name'):
                    current_task_name = row['task_name']
                DB.add_row(row)
                #import epdb; epdb.st()


    DB.process()
    DB.print_detailed_tree(dest=os.path.join(args.directory, 'pidtree.txt'))
    #DB.print_fork_timeseries(dest=os.path.join(args.directory, 'forktime.txt'))
    DB.print_fork_timeseries(dest=os.path.join(args.directory, 'forktime_abs.txt'), timescale=125)
    #DB.select(ppid=None, pid=5976, task_name=None, host=None, strace=None, raw=True)
    #import epdb; epdb.st()


if __name__ == "__main__":
    main()
